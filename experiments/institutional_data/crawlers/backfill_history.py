#!/usr/bin/env python3
"""
ä¸‰å¤§æ³•äººæ­·å²æ•¸æ“šè£œé½Šè…³æœ¬

åŠŸèƒ½:
1. æ‰¹æ¬¡è£œé½ŠæŒ‡å®šæ—¥æœŸç¯„åœçš„ä¸‰å¤§æ³•äººæ•¸æ“š
2. è‡ªå‹•è·³éé€±æœ«èˆ‡å‡æ—¥
3. é¿å…é‡è¤‡çˆ¬å–
4. æ”¯æ´æ–·é»çºŒå‚³

ä½¿ç”¨:
    python backfill_history.py --start 2020-01-01 --end 2024-11-21
    python backfill_history.py --start 2024-01-01  # end é è¨­ä»Šå¤©
"""

import sys
import os
from pathlib import Path
import argparse
from datetime import datetime, timedelta
import time
import pandas as pd

# åŠ å…¥ä¸Šå±¤ç›®éŒ„ä»¥import fetch_institution
sys.path.append(str(Path(__file__).parent))
from fetch_institution import fetch_and_save, RAW_DATA_DIR


def is_weekend(date_obj):
    """æª¢æŸ¥æ˜¯å¦ç‚ºé€±æœ«"""
    return date_obj.weekday() >= 5  # 5=å…­, 6=æ—¥


def file_exists(date_str):
    """æª¢æŸ¥è©²æ—¥æœŸçš„æ•¸æ“šæª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨"""
    filename = f"institution_{date_str}.csv"
    filepath = RAW_DATA_DIR / filename
    return filepath.exists()


def backfill_historical_data(start_date, end_date, skip_existing=True, sleep_seconds=3):
    """
    æ‰¹æ¬¡è£œé½Šæ­·å²æ•¸æ“š
    
    Args:
        start_date: èµ·å§‹æ—¥æœŸ 'YYYY-MM-DD'
        end_date: çµæŸæ—¥æœŸ 'YYYY-MM-DD'
        skip_existing: æ˜¯å¦è·³éå·²å­˜åœ¨çš„æª”æ¡ˆ
        sleep_seconds: æ¯æ¬¡è«‹æ±‚é–“éš”ç§’æ•¸
    
    Returns:
        dict: çµ±è¨ˆè³‡è¨Š
    """
    start_obj = datetime.strptime(start_date, '%Y-%m-%d')
    end_obj = datetime.strptime(end_date, '%Y-%m-%d')
    
    if start_obj > end_obj:
        print("âŒ èµ·å§‹æ—¥æœŸä¸èƒ½æ™šæ–¼çµæŸæ—¥æœŸ")
        return None
    
    print(f"\n{'='*70}")
    print(f"æ‰¹æ¬¡è£œé½Šä¸‰å¤§æ³•äººæ­·å²æ•¸æ“š")
    print(f"{'='*70}")
    print(f"èµ·å§‹æ—¥æœŸ: {start_date}")
    print(f"çµæŸæ—¥æœŸ: {end_date}")
    print(f"è·³éå·²å­˜åœ¨: {skip_existing}")
    print(f"è«‹æ±‚é–“éš”: {sleep_seconds} ç§’")
    print(f"{'='*70}\n")
    
    # çµ±è¨ˆè³‡è¨Š
    stats = {
        'total_days': 0,
        'success': 0,
        'failed': 0,
        'skipped_weekend': 0,
        'skipped_existing': 0
    }
    
    current_date = start_obj
    
    while current_date <= end_obj:
        date_str = current_date.strftime('%Y-%m-%d')
        stats['total_days'] += 1
        
        # è·³éé€±æœ«
        if is_weekend(current_date):
            print(f"â­ï¸  {date_str} (é€±æœ«ï¼Œè·³é)")
            stats['skipped_weekend'] += 1
            current_date += timedelta(days=1)
            continue
        
        # è·³éå·²å­˜åœ¨çš„æª”æ¡ˆ
        if skip_existing and file_exists(date_str):
            print(f"âœ… {date_str} (å·²å­˜åœ¨ï¼Œè·³é)")
            stats['skipped_existing'] += 1
            current_date += timedelta(days=1)
            continue
        
        # çˆ¬å–æ•¸æ“š
        try:
            success = fetch_and_save(date_str)
            
            if success:
                stats['success'] += 1
                print(f"âœ… {date_str} - æˆåŠŸ ({stats['success']}/{stats['total_days'] - stats['skipped_weekend'] - stats['skipped_existing']})")
            else:
                stats['failed'] += 1
                print(f"âŒ {date_str} - å¤±æ•— (å¯èƒ½ç‚ºå‡æ—¥)")
            
            # é¿å…è«‹æ±‚éå¿«
            time.sleep(sleep_seconds)
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·çˆ¬å–")
            print(f"å·²è™•ç†åˆ°: {date_str}")
            break
        except Exception as e:
            print(f"âŒ {date_str} - éŒ¯èª¤: {e}")
            stats['failed'] += 1
            time.sleep(sleep_seconds)
        
        current_date += timedelta(days=1)
    
    # åˆ—å°çµ±è¨ˆ
    print(f"\n{'='*70}")
    print("è£œé½Šå®Œæˆï¼")
    print(f"{'='*70}")
    print(f"ç¸½å¤©æ•¸: {stats['total_days']}")
    print(f"æˆåŠŸ: {stats['success']}")
    print(f"å¤±æ•—: {stats['failed']}")
    print(f"è·³éï¼ˆé€±æœ«ï¼‰: {stats['skipped_weekend']}")
    print(f"è·³éï¼ˆå·²å­˜åœ¨ï¼‰: {stats['skipped_existing']}")
    print(f"{'='*70}\n")
    
    return stats


def consolidate_data():
    """
    å°‡æ‰€æœ‰å–®æ—¥æª”æ¡ˆåˆä½µç‚ºä¸€å€‹å®Œæ•´æª”æ¡ˆ
    
    Returns:
        DataFrame or None
    """
    print("\nğŸ“Š åˆä½µæ‰€æœ‰æ•¸æ“šæª”æ¡ˆ...")
    
    csv_files = sorted(RAW_DATA_DIR.glob("institution_*.csv"))
    
    if not csv_files:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•æ•¸æ“šæª”æ¡ˆ")
        return None
    
    print(f"æ‰¾åˆ° {len(csv_files)} å€‹æª”æ¡ˆ")
    
    dfs = []
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            dfs.append(df)
        except Exception as e:
            print(f"âš ï¸  è®€å–å¤±æ•—: {csv_file.name} - {e}")
    
    if not dfs:
        print("âŒ ç„¡æ³•è®€å–ä»»ä½•æª”æ¡ˆ")
        return None
    
    df_combined = pd.concat(dfs, ignore_index=True)
    
    # æ’åº
    df_combined = df_combined.sort_values(['date', 'sid']).reset_index(drop=True)
    
    # å„²å­˜åˆä½µæª”æ¡ˆ
    output_file = RAW_DATA_DIR.parent / 'processed' / 'institution_all.csv'
    output_file.parent.mkdir(parents=True, exist_ok=True)
    df_combined.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    print(f"âœ… åˆä½µå®Œæˆ: {output_file}")
    print(f"   ç¸½ç­†æ•¸: {len(df_combined):,}")
    print(f"   æ—¥æœŸç¯„åœ: {df_combined['date'].min()} ~ {df_combined['date'].max()}")
    print(f"   è‚¡ç¥¨æ•¸: {df_combined['sid'].nunique():,}")
    
    return df_combined


def main():
    parser = argparse.ArgumentParser(description='æ‰¹æ¬¡è£œé½Šä¸‰å¤§æ³•äººæ­·å²æ•¸æ“š')
    parser.add_argument('--start', type=str, required=True,
                       help='èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end', type=str, 
                       default=datetime.now().strftime('%Y-%m-%d'),
                       help='çµæŸæ—¥æœŸ (YYYY-MM-DD), é è¨­ä»Šå¤©')
    parser.add_argument('--skip-existing', action='store_true', default=True,
                       help='è·³éå·²å­˜åœ¨çš„æª”æ¡ˆ (é è¨­é–‹å•Ÿ)')
    parser.add_argument('--no-skip-existing', dest='skip_existing', action='store_false',
                       help='é‡æ–°çˆ¬å–å·²å­˜åœ¨çš„æª”æ¡ˆ')
    parser.add_argument('--sleep', type=int, default=3,
                       help='æ¯æ¬¡è«‹æ±‚é–“éš”ç§’æ•¸ (é è¨­ 3 ç§’)')
    parser.add_argument('--consolidate', action='store_true',
                       help='è£œé½Šå¾Œåˆä½µæ‰€æœ‰æ•¸æ“š')
    
    args = parser.parse_args()
    
    # é©—è­‰æ—¥æœŸæ ¼å¼
    try:
        datetime.strptime(args.start, '%Y-%m-%d')
        datetime.strptime(args.end, '%Y-%m-%d')
    except ValueError:
        print("âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY-MM-DD")
        exit(1)
    
    # é–‹å§‹è£œé½Š
    stats = backfill_historical_data(
        args.start, 
        args.end, 
        skip_existing=args.skip_existing,
        sleep_seconds=args.sleep
    )
    
    if stats is None:
        print("\nâŒ è£œé½Šå¤±æ•—")
        exit(1)
    
    # åˆä½µæ•¸æ“š
    if args.consolidate and stats['success'] > 0:
        consolidate_data()
    
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    main()
