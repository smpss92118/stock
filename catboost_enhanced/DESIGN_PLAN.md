# CatBoost Enhanced - é‡æ„è®¾è®¡è§„åˆ’ (v1.0)

**æ—¥æœŸ**: 2025-11-22
**ç›®æ ‡**: ä»ç¢ç‰‡åŒ–çš„ 9 æ¨¡å‹ç³»ç»Ÿå‡çº§åˆ°å…¨å±€ CatBoost æ¨¡å‹ï¼Œèåˆä¸‰å¤§æ ¸å¿ƒæ”¹è¿›ï¼šP0ï¼ˆå…¨å±€ï¼‰ã€P1ï¼ˆæ•°æ®éš”ç¦»ï¼‰ã€P2ï¼ˆç›®æ ‡å‡½æ•°ï¼‰

---

## ğŸ“‹ Executive Summary

### ç°çŠ¶é—®é¢˜
- **ml_enhanced**: æŒ‰ pattern Ã— exit_mode åˆ‡åˆ†æˆ 9 ä¸ªç‹¬ç«‹ XGBoost æ¨¡å‹ï¼Œæ¯ä¸ªä»… ~3000 æ ·æœ¬ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
- **catboost_enhanced**: å·²æœ‰å…¨å±€æ¨¡å‹æ¡†æ¶ï¼Œä½†ç¼ºå°‘ä¸¥æ ¼çš„æ•°æ®éš”ç¦»ã€ç¼ºä¹è¯„åˆ†æ„ŸçŸ¥çš„æƒé‡è®¾è®¡
- **ç‰¹å¾ä¸æ ‡ç­¾**: ä¸¤å¥—ç³»ç»Ÿçš„ score è®¡ç®—å’Œæ ‡ç­¾ç”Ÿæˆé€»è¾‘é‡å¤ï¼Œç‰¹å¾å¯¹é½å­˜ç–‘

### æ”¹è¿›ç›®æ ‡
| ç»´åº¦ | å½“å‰ | ç›®æ ‡ | é¢„æœŸæ”¶ç›Š |
|------|-----|------|--------|
| **P0: æ ·æœ¬é‡** | 3000/model | 36,823/model | è¿‡æ‹Ÿåˆâ†“, ç¨³å®šæ€§â†‘ |
| **P1: æ•°æ®éš”ç¦»** | walk-forward æ—  embargo | PurgedGroupKFold (20day embargo) | Label leak â†“ |
| **P2: ç›®æ ‡å‡½æ•°** | ç®€å•äºŒåˆ†ç±» + class weight | Ordinal + score-aware weight + class weight | A ç±»æ•æ‰â†‘, è™šå‡ä¿¡å·â†“ |
| **éƒ¨ç½²** | ä¸¤å¥—ç‹¬ç«‹ç³»ç»Ÿ | å•ä¸€å…¨å±€æ¨¡å‹ + ä¸¤å¥—æŠ¥å‘Š | ç»´æŠ¤æˆæœ¬â†“, ä¸€è‡´æ€§â†‘ |

---

## ğŸ¯ ä¸‰å¤§æ ¸å¿ƒæ”¹è¿›ï¼ˆä¼˜å…ˆçº§ï¼‰

### **P0: å…¨å±€æ¨¡å‹ (Global Model)**

#### é—®é¢˜åˆ†æ
```
å½“å‰ ml_enhanced:
  for pattern in ['HTF', 'CUP', 'VCP']:
      for exit in ['fixed_r2_t20', 'fixed_r3_t20', 'trailing_15r']:
          # è®­ç»ƒ 1 ä¸ªæ¨¡å‹ (pattern Ã— exit)
          # æ ·æœ¬æ•°: ~3000 (æ ·æœ¬å°‘ â†’ æ˜“è¿‡æ‹Ÿåˆ)
          # ç‰¹å¾æ˜¯å¦å¯¹é½? â†’ ä¸æ¸…æ¥š

ç›®æ ‡ catboost_enhanced:
  # è®­ç»ƒ 1 ä¸ªå…¨å±€æ¨¡å‹
  # pattern_type, exit_mode â†’ categorical features (ä¸åˆ†å‰²æ•°æ®)
  # æ ·æœ¬æ•°: 36,823 (3000Ã—9 é€šè¿‡ categorical encoding)
  # å­¦ä¹ èƒ½åŠ›: è·¨ pattern å­¦ä¹ å…±æ€§ + è·¨ exit å­¦ä¹ æœ€ä¼˜ç»„åˆ
```

#### è®¾è®¡æ–¹æ¡ˆ

**æ–¹æ¡ˆA: CatBoostClassifier (æ¨è)**
```python
# catboost_enhanced/scripts/train.py

from catboost import CatBoostClassifier, Pool

model = CatBoostClassifier(
    # === åŸºç¡€é…ç½® ===
    loss_function='MultiClass',
    classes_for_smooth_cat=4,  # D=0, C=1, B=2, A=3

    # === ç±»åˆ«ç‰¹å¾ (é¿å…æ•°å€¼åŒ–) ===
    cat_features=['pattern_type', 'exit_mode', 'ma_trend'],

    # === ç±»æƒé‡ (åŸºç¡€) ===
    class_weights=[1.0, 1.0, 1.5, 2.0],  # D, C, B, A

    # === è¶…å‚æ•° ===
    iterations=2000,
    learning_rate=0.05,
    depth=6,
    verbose=100,

    # === æ•°æ®æ³„æ¼é˜²æŠ¤ ===
    # äº¤å‰éªŒè¯é‡‡ç”¨ PurgedGroupKFold (è§ P1)
)

# ç‰¹åˆ«: æ ·æœ¬æƒé‡æ¥è‡ª P2 (è§ä¸‹)
model.fit(
    X_train, y_train,
    sample_weight=compute_sample_weights(df_train),  # â­ å…³é”®
    eval_set=[(X_test, y_test)],
)
```

**é¢„æœŸæ”¶ç›Š**:
- æ ·æœ¬é‡ 3000 â†’ 36,823 (12å€), è¿‡æ‹Ÿåˆå¤§å¹…ä¸‹é™
- CatBoost å¯¹ categorical feature çš„å¤„ç†ä¼˜äº one-hot encoding
- è·¨ pattern å­¦ä¹ : ä¾‹å¦‚"é«˜æˆäº¤é‡é€šå¸¸åˆ©å¥½"é€‚ç”¨äºæ‰€æœ‰å½¢æ€
- è·¨ exit å­¦ä¹ : æ‰¾åˆ°æœ€ä¼˜çš„ exit_mode ç»„åˆ

---

### **P1: æ•°æ®éš”ç¦» (Embargo/Purging)**

#### é—®é¢˜åˆ†æ
```
å½“å‰ ml_enhanced (train_models.py):
  # æ—¶é—´åºåˆ—åˆ†å‰²
  split_idx = len(df) * 0.8  # 80% train, 20% test
  train_df = df.iloc[:split_idx]
  test_df = df.iloc[split_idx:]

  # é—®é¢˜: æ²¡æœ‰ embargo
  # - train æœ€åä¸€å¤©å’Œ test ç¬¬ä¸€å¤©ï¼Œæ•°æ®å¯èƒ½é‡å 
  # - label = "æœªæ¥ 20 å¤©æ”¶ç›Šç‡" â†’ test çš„ feature å¯èƒ½"çœ‹åˆ°"train çš„æœªæ¥
  # - Walk-forward çª—å£é—´å¯èƒ½æœ‰ label leak

ç›®æ ‡:
  # æ—¥æœŸç²’åº¦çš„ä¸¥æ ¼éš”ç¦»
  # test çš„ç¬¬ä¸€å¤© â‰¥ train çš„æœ€åä¸€å¤© + 20å¤©(embargo)
  # åŸå› : predict horizon = 20 days, éœ€è¦é¢å¤– buffer
```

#### è®¾è®¡æ–¹æ¡ˆ

**å®ç° PurgedGroupKFold**
```python
# catboost_enhanced/utils/data_splitter.py

class PurgedGroupKFold:
    def __init__(self, n_splits=5, embargo_pct=0.05):
        """
        n_splits: äº¤å‰éªŒè¯æŠ˜æ•° (é»˜è®¤ 5)
        embargo_pct: embargo å æ€»æ—¥æœŸæ•°çš„æ¯”ä¾‹
                     å‡è®¾ 20 å¤© embargo, æ€» ~400 trading days/year
                     â†’ embargo_pct â‰ˆ 20/400 = 0.05 (5%)
        """
        self.n_splits = n_splits
        self.embargo_pct = embargo_pct

    def split(self, df, groups=None):
        """
        df: æ•°æ®é›† (åŒ…å« 'date' åˆ—)
        groups: df['date'] (æ¯è¡Œçš„äº¤æ˜“æ—¥æœŸ)

        yield: (train_indices, test_indices)

        ä¸å˜é‡:
        1. max(train_date) + embargo_days <= min(test_date)
        2. æ‰€æœ‰ fold éƒ½æ»¡è¶³ä¸Šè¿°æ¡ä»¶
        """
        unique_dates = sorted(df[groups].unique())
        n_dates = len(unique_dates)
        embargo_size = max(1, int(n_dates * self.embargo_pct))

        # Walk-forward åˆ†å‰²
        for fold_idx in range(self.n_splits):
            # æµ‹è¯•é›†èŒƒå›´ (æ—¥æœŸç´¢å¼•)
            test_start_idx = fold_idx * n_dates // self.n_splits
            test_end_idx = (fold_idx + 1) * n_dates // self.n_splits

            # è®­ç»ƒé›†èŒƒå›´ (åº”ç”¨ embargo)
            train_end_idx = test_start_idx - embargo_size

            if train_end_idx <= 0:
                continue  # ç¬¬ä¸€ä¸ª fold å¯èƒ½æ²¡æœ‰è®­ç»ƒæ•°æ®

            train_dates = unique_dates[:train_end_idx]
            test_dates = unique_dates[test_start_idx:test_end_idx]

            # è½¬æ¢å›æ ·æœ¬ç´¢å¼•
            train_indices = df[df['date'].isin(train_dates)].index.to_numpy()
            test_indices = df[df['date'].isin(test_dates)].index.to_numpy()

            yield train_indices, test_indices

# ä½¿ç”¨ç¤ºä¾‹:
from catboost_enhanced.utils.data_splitter import PurgedGroupKFold

cv = PurgedGroupKFold(n_splits=5, embargo_pct=0.05)

for fold, (train_idx, test_idx) in enumerate(cv.split(df, groups=df['date'])):
    # éªŒè¯: æ²¡æœ‰æ•°æ®æ³„æ¼
    last_train_date = df.iloc[train_idx]['date'].max()
    first_test_date = df.iloc[test_idx]['date'].min()
    gap = (first_test_date - last_train_date).days

    assert gap >= embargo_days, f"Fold {fold}: gap={gap}å¤©, å°äº embargo_days={embargo_days}"
    print(f"âœ“ Fold {fold}: gap={gap}å¤© (ç¬¦åˆ embargo è¦æ±‚)")

    # è®­ç»ƒ
    X_train, y_train = df.iloc[train_idx][feature_cols], df.iloc[train_idx]['label']
    X_test, y_test = df.iloc[test_idx][feature_cols], df.iloc[test_idx]['label']

    model.fit(X_train, y_train)
    # ...
```

**å‚æ•°å»ºè®®**:
```python
# åŸºäºæ‚¨çš„è®¾ç½®
embargo_days = 20  # é¢„æµ‹çª—å£é•¿åº¦
trading_days_per_year = 240  # ~1å¹´ trading days
total_years = 1.5  # æ•°æ®è·¨åº¦
total_trading_days = trading_days_per_year * total_years â‰ˆ 360

embargo_pct = embargo_days / total_trading_days â‰ˆ 0.056 â‰ˆ 0.05
```

**é¢„æœŸæ”¶ç›Š**:
- æ¶ˆé™¤ label leak: test feature ä¸ä¼š"çœ‹åˆ°"train future
- å›æµ‹ç»“æœçœŸå®å¯ä¿¡ (Sharpe, Sortino ä¸ä¼šè™šé«˜)
- CV folds é—´çš„ generalization gap æ›´å° (å¦‚æœæ¨¡å‹çœŸçš„æœ‰æ•ˆ)

---

### **P2: ç›®æ ‡å‡½æ•°ä¸æ ·æœ¬æƒé‡**

#### é—®é¢˜åˆ†æ
```
å½“å‰ç›®æ ‡å˜é‡è®¾è®¡:
  is_winner = 1 if label in ['A', 'B'] else 0
  â†’ äºŒåˆ†ç±»é—®é¢˜

é—®é¢˜:
1. ä¿¡æ¯æŸå¤±: ä¸¢å¼ƒäº† A/B çš„é¡ºåºã€C/D çš„é¡ºåº
   - é¢„æµ‹ B å’Œé¢„æµ‹ A çš„ä»·å€¼ä¸åŒï¼Œä½†æŸå¤±ç›¸åŒ

2. ç±»åˆ«ä¸å¹³è¡¡: A ç±»æå°‘ (~10-15%)
   - æ¨¡å‹å€¾å‘äºé¢„æµ‹ D (æœ€ä¿å®ˆ)
   - é«˜åˆ† A ç±»ä¿¡å·è¢«æ·¹æ²¡

3. å¿½è§†"æ•ˆç‡": score çš„ç»å¯¹å€¼è¢«å¿½ç•¥
   - score = -10%/day å’Œ score = 0%/day åŒç­‰å¯¹å¾…
   - é«˜é£é™©äº¤æ˜“åº”è¯¥æœ‰æ›´é«˜çš„å­¦ä¹ æƒé‡ (regardless èµ¢è¾“)

æ”¹è¿›ç›®æ ‡:
  1. å››åˆ†ç±» (ABCD) + ordinal loss: A > B > C > D
  2. æ ·æœ¬æƒé‡åŸºäº |score|: å¼ºåˆ¶å…³æ³¨"é«˜æ•ˆç‡äº¤æ˜“"
  3. ç±»æƒé‡: A/B æƒé‡ > C/D (åæ˜ ç¨€æœ‰æ€§)
```

#### è®¾è®¡æ–¹æ¡ˆ

**1. å¤šå±‚æŸå¤±å‡½æ•°**

```python
# catboost_enhanced/utils/loss_functions.py

def compute_sample_weights(df_train):
    """
    æ ¹æ® score çš„ç»å¯¹å€¼ + æ ‡ç­¾è®¡ç®—æ ·æœ¬æƒé‡

    è®¾è®¡åŸç†:
    1. score å¹…åº¦æƒé‡: |score| è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜
       - åŸå› : é«˜æ•ˆç‡äº¤æ˜“ (æ­£æˆ–è´Ÿ) æ¯”ä½æ•ˆç‡æ›´å€¼å¾—å­¦ä¹ 
       - ä½¿ç”¨ sigmoid é¿å…æç«¯å€¼ä¸»å¯¼

    2. æ ‡ç­¾æƒé‡: A > B > C â‰¥ D
       - åŸå› : ç¨€æœ‰æ€§å’Œå­¦ä¹ ä»·å€¼ä¸åŒ

    3. ç±»åˆ«å¹³è¡¡è¡¥å¿: æŒ‰ç…§ç±»é¢‘ç‡å¼€å¹³æ–¹æ ¹è°ƒæ•´
       - åŸå› : é¿å…è¿‡åº¦è¡¥å¿

    4. æ ‡å‡†åŒ–: ä½¿å¾— mean(weights) = 1
    """

    # === ç¬¬ä¸€å±‚: åŸºäº |score| çš„å¹…åº¦ ===
    # sigmoid(|score| * 2) æ˜ å°„åˆ° [0.5, 1)
    score_magnitude = np.abs(df_train['score'])
    score_weights = 1 / (1 + np.exp(-score_magnitude * 2))

    # === ç¬¬äºŒå±‚: åŸºäºæ ‡ç­¾ ===
    label_weight_map = {
        0: 1.0,   # D: åº•éƒ¨
        1: 1.0,   # C: ä¸­ä¸‹
        2: 1.5,   # B: ä¸­ä¸Š (ç¨€æœ‰+é‡è¦)
        3: 2.0,   # A: é¡¶éƒ¨ (æœ€ç¨€æœ‰+æœ€é‡è¦)
    }
    class_weights = df_train['label'].map(label_weight_map)

    # === ç¬¬ä¸‰å±‚: ç»“åˆä¸¤å±‚ ===
    combined_weights = score_weights * class_weights

    # === ç¬¬å››å±‚: ç±»åˆ«å¹³è¡¡è¡¥å¿ ===
    for label in [0, 1, 2, 3]:
        mask = df_train['label'] == label
        count = mask.sum()
        if count > 0:
            # é¢‘ç¹çš„ç±» (C/D) æƒé‡ä¸‹è°ƒ; ç¨€æœ‰çš„ç±» (A/B) æƒé‡ä¸Šè°ƒ
            combined_weights[mask] /= np.sqrt(count)

    # === æ ‡å‡†åŒ– ===
    return combined_weights / combined_weights.mean()


def verify_sample_weights(weights, df_train):
    """éªŒè¯æƒé‡åˆ†å¸ƒæ˜¯å¦åˆç†"""
    print("æ ·æœ¬æƒé‡ç»Ÿè®¡:")
    print(f"  Mean: {weights.mean():.4f} (åº”æ¥è¿‘ 1.0)")
    print(f"  Std:  {weights.std():.4f}")
    print(f"  Min:  {weights.min():.4f}")
    print(f"  Max:  {weights.max():.4f}")

    for label in [0, 1, 2, 3]:
        mask = df_train['label'] == label
        print(f"  Label {label} (n={mask.sum()}):"
              f" mean={weights[mask].mean():.4f}, "
              f"max={weights[mask].max():.4f}")
```

**2. Ordinal Loss (å¯é€‰è¿›é˜¶)**

```python
# åŸç†: å¦‚æœçœŸå®æ˜¯ D (0) ä½†é¢„æµ‹ä¸º A (3),
#       rank distance = 3, loss æ›´å¤§
#       å¦‚æœé¢„æµ‹ä¸º C (1), rank distance = 1, loss æ›´å°

# å®ç°æ–¹æ³• (CatBoost æ— å†…ç½® ordinal loss, ä½†å¯ä»¥é€šè¿‡ custom metric):
def ordinal_loss(y_true, y_pred):
    """Ordinal: A (3) > B (2) > C (1) > D (0)"""
    rank_distance = np.abs(y_true - y_pred)  # L1 distance
    return rank_distance  # CatBoost ä¼šè®¡ç®—å¹³å‡

# åœ¨ CatBoost ä¸­ä½¿ç”¨è‡ªå®šä¹‰ loss:
# model = CatBoostClassifier(
#     loss_function='MultiClass',  # ä¸»æŸå¤±
#     custom_loss=['Accuracy'],     # è¾…åŠ© metric
# )

# æˆ–è€…, é€šè¿‡ä¿®æ”¹ sample_weight æ¨¡æ‹Ÿ ordinal æ•ˆæœ:
def simulate_ordinal_with_weights(df_train, base_weights):
    """
    é€šè¿‡åŠ æƒæ¥æ¨¡æ‹Ÿ ordinal loss
    å¦‚æœæ•°æ®ä¸­å­˜åœ¨å¾ˆå¤š"é¢„æµ‹é”™è¯¯ 1 çº§åˆ«" vs "é¢„æµ‹é”™è¯¯ 3 çº§åˆ«",
    å¯ä»¥åœ¨å‡†å¤‡æ•°æ®æ—¶å°±è°ƒæ•´æƒé‡ã€‚

    æ­¤å¤„ç®€åŒ–: ä»…ç”¨åŸºç¡€çš„ sample_weight
    """
    return base_weights
```

**3. CatBoost é…ç½®**

```python
# catboost_enhanced/configs/model_config.py

CATBOOST_PARAMS = {
    # === åŸºç¡€ ===
    'loss_function': 'MultiClass',
    'classes_for_smooth_cat': 4,

    # === ç±»åˆ«ç‰¹å¾ (é¿å… one-hot encoding) ===
    'cat_features': ['pattern_type', 'exit_mode', 'ma_trend'],

    # === ç±»æƒé‡ (åŸºç¡€, ä¸è€ƒè™‘ sample_weight) ===
    'class_weights': [1.0, 1.0, 1.5, 2.0],  # D, C, B, A

    # === è¶…å‚æ•° ===
    'iterations': 2000,
    'learning_rate': 0.05,
    'depth': 6,
    'l2_leaf_reg': 3,
    'bagging_temperature': 1.0,
    'random_strength': 1,

    # === éªŒè¯ ===
    'eval_metric': 'MultiClass',
    'verbose': 100,
    'early_stopping_rounds': 100,
}

# åœ¨è®­ç»ƒæ—¶åº”ç”¨:
model = CatBoostClassifier(**CATBOOST_PARAMS)
model.fit(
    X_train, y_train,
    sample_weight=compute_sample_weights(df_train),  # â­ å…³é”®
    eval_set=[(X_test, y_test)],
    # eval_set çš„æƒé‡ä¸ä¼  (åªç”¨äºç›‘æ§, ä¸æ›´æ–°æƒé‡)
)
```

**é¢„æœŸæ”¶ç›Š**:
- A ç±»æ•æ‰ç‡æå‡ (ä» baseline ~50% æå‡åˆ° 60%+)
- è™šå‡ä¿¡å·å‡å°‘ (D ç±»è¢«è¯¯åˆ†ä¸º A çš„æ¦‚ç‡ä¸‹é™)
- æ¨¡å‹æ›´å…³æ³¨"é«˜æ•ˆç‡äº¤æ˜“"è€Œé"éƒ½é¢„æµ‹ C" çš„æ‡’æƒ°ç­–ç•¥

---

## ğŸ—ï¸ ç›®å½•é‡æ„è®¾è®¡

### æ–°çš„ catboost_enhanced ç»“æ„

```
/Users/sony/ml_stock/stock/catboost_enhanced/
â”œâ”€â”€ README.md                              # ç³»ç»Ÿè¯´æ˜
â”œâ”€â”€ DESIGN_PLAN.md                         # æœ¬æ–‡æ¡£
â”‚
â”œâ”€â”€ configs/                               # â­ æ–°å¢: é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ constants.py                       # Pattern/Exit/Label å¸¸é‡
â”‚   â”œâ”€â”€ model_config.py                    # æ¨¡å‹å‚æ•° (weight/loss)
â”‚   â””â”€â”€ feature_config.py                  # ç‰¹å¾åˆ—è¡¨
â”‚
â”œâ”€â”€ scripts/                               # æ ¸å¿ƒè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prepare_catboost_data.py           # â­ æ–°å¢: æ•°æ®å‡†å¤‡ (å¤ç”¨ ml_enhanced)
â”‚   â”œâ”€â”€ train.py                           # â­ ä¼˜åŒ–: P0+P1+P2 å…¨å±€è®­ç»ƒ
â”‚   â”œâ”€â”€ run_catboost_backtest.py           # â­ æ–°å¢: å›æµ‹éªŒè¯
â”‚   â””â”€â”€ daily_scan.py                      # â­ ä¼˜åŒ–: æ—¥å¸¸é¢„æµ‹
â”‚
â”œâ”€â”€ utils/                                 # â­ æ–°å¢/æ‰©å±•: è¾…åŠ©å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_splitter.py                   # PurgedGroupKFold
â”‚   â”œâ”€â”€ loss_functions.py                  # compute_sample_weights etc
â”‚   â”œâ”€â”€ metrics.py                         # è¯„ä¼°æŒ‡æ ‡ (NDCG, AUCç­‰)
â”‚   â””â”€â”€ feature_alignment.py               # ç‰¹å¾å¯¹é½æ£€æŸ¥
â”‚
â”œâ”€â”€ weekly_retrain.py                      # â­ æ–°å¢: å‘¨æœŸè‡ªåŠ¨åŒ– (ä¸¤å¥—æ¨¡å‹)
â”œâ”€â”€ daily_ml_scanner.py                    # â­ æ–°å¢: æ—¥å¸¸æ‰«æ (ä¸¤å¥—æŠ¥å‘Š)
â”‚
â”œâ”€â”€ models/                                # æ¨¡å‹å­˜å‚¨
â”‚   â”œâ”€â”€ catboost_global.cbm                # å…¨å±€æ¨¡å‹
â”‚   â”œâ”€â”€ feature_info.pkl                   # ç‰¹å¾å…ƒæ•°æ®
â”‚   â””â”€â”€ model_metrics.json                 # è®­ç»ƒæŒ‡æ ‡
â”‚
â”œâ”€â”€ data/                                  # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ catboost_features.csv              # å®Œæ•´ç‰¹å¾é›†
â”‚   â”œâ”€â”€ train_indices.pkl                  # è®­ç»ƒç´¢å¼•
â”‚   â””â”€â”€ test_indices.pkl                   # æµ‹è¯•ç´¢å¼•
â”‚
â””â”€â”€ results/                               # è¾“å‡ºç»“æœ
    â”œâ”€â”€ feature_importance.csv
    â”œâ”€â”€ cv_metrics.csv
    â”œâ”€â”€ backtest_results.csv
    â””â”€â”€ daily_reports/
        â””â”€â”€ YYYY-MM-DD/
            â””â”€â”€ catboost_daily_summary.md
```

### ä¸ ml_enhanced å…±äº«çš„æ¨¡å—

éœ€è¦æŠ½å–åˆ° `src/ml/` çš„é€šç”¨éƒ¨åˆ†:

```python
# src/ml/constants.py (æ–°å»º)
PATTERN_TYPES = ['htf', 'cup', 'vcp']
EXIT_MODES = ['fixed_r2_t20', 'fixed_r3_t20', 'trailing_15r']
LABEL_RULES = {
    'A': (0.75, 1.0),   # Q75 - Q100
    'B': (0.50, 0.75),  # Q50 - Q75
    'C': (0.25, 0.50),  # Q25 - Q50
    'D': (0.00, 0.25),  # Q0 - Q25
}

# src/ml/labeling.py (æ–°å»º)
def compute_score(profit_pct, holding_days):
    """è®¡ç®—æ•ˆç‡åˆ†æ•°"""
    return (profit_pct * 100) / holding_days

def assign_label(score, q25, q50, q75):
    """åŸºäºå››åˆ†ä½æ•°åˆ†é…æ ‡ç­¾"""
    if score >= q75: return 'A'
    elif score >= q50: return 'B'
    elif score >= q25: return 'C'
    else: return 'D'

# src/ml/simulation.py (æ–°å»º)
def simulate_trade_fixed(...): ...
def simulate_trade_trailing(...): ...
```

---

## ğŸ“ ä»»åŠ¡æ¸…å•ä¸ä¼˜å…ˆçº§

### ç¬¬ä¸€é˜¶æ®µ: åŸºç¡€è®¾ç½® (Week 1)

- [ ] **Task 1.1**: åˆ›å»º `src/ml/constants.py` å’Œ `src/ml/labeling.py`
  - è¾“å‡º: ç»Ÿä¸€çš„ PATTERN_TYPES, EXIT_MODES, æ ‡ç­¾ç”Ÿæˆå‡½æ•°
  - ä¾èµ–: æ— 

- [ ] **Task 1.2**: åˆ›å»º `catboost_enhanced/configs/` ç›®å½•ç»“æ„
  - åˆ›å»º `constants.py`, `model_config.py`, `feature_config.py`
  - è¾“å‡º: é…ç½®æ–‡ä»¶ + æ¨¡å‹å‚æ•°ä¸­å¤®ç®¡ç†
  - ä¾èµ–: Task 1.1

- [ ] **Task 1.3**: åˆ›å»º `catboost_enhanced/utils/` å¹¶å®ç° core å‡½æ•°
  - `data_splitter.py` (PurgedGroupKFold)
  - `loss_functions.py` (compute_sample_weights)
  - `metrics.py` (è¯„ä¼°æŒ‡æ ‡)
  - è¾“å‡º: å¯é‡ç”¨çš„å·¥å…·åº“
  - ä¾èµ–: Task 1.1, 1.2

### ç¬¬äºŒé˜¶æ®µ: æ ¸å¿ƒè®­ç»ƒç®¡é“ (Week 2-3)

- [ ] **Task 2.1**: åˆ›å»º `catboost_enhanced/scripts/prepare_catboost_data.py`
  - å¤ç”¨ `ml_enhanced/scripts/prepare_ml_data.py` çš„é€»è¾‘
  - è¾“å‡º: `catboost_enhanced/data/catboost_features.csv`
  - ä¾èµ–: Task 1.1, 1.3

- [ ] **Task 2.2**: ä¼˜åŒ– `catboost_enhanced/scripts/train.py`
  - å®ç° P0 (å…¨å±€æ¨¡å‹, pattern/exit ä½œä¸ºç‰¹å¾)
  - å®ç° P1 (PurgedGroupKFold + embargo)
  - å®ç° P2 (æ ·æœ¬æƒé‡ + ç±»æƒé‡)
  - è¾“å‡º: `catboost_global.cbm` + `feature_info.pkl`
  - ä¾èµ–: Task 2.1, 1.3

- [ ] **Task 2.3**: åˆ›å»º `catboost_enhanced/scripts/run_catboost_backtest.py`
  - å®ç°ä¸ `ml_enhanced/run_ml_backtest.py` ä¸€è‡´çš„å›æµ‹é€»è¾‘
  - è¾“å‡º: `catboost_backtest_results.csv`
  - ä¾èµ–: Task 2.2

### ç¬¬ä¸‰é˜¶æ®µ: è‡ªåŠ¨åŒ–ä¸æŠ¥å‘Š (Week 3-4)

- [ ] **Task 3.1**: åˆ›å»º `catboost_enhanced/weekly_retrain.py`
  - è°ƒç”¨ prepare â†’ train â†’ backtest
  - åŒæ—¶æ›´æ–°ä¸¤å¥—æ¨¡å‹ (ml_enhanced + catboost_enhanced)
  - è¾“å‡º: æ¨¡å‹æƒé‡ + æŒ‡æ ‡
  - ä¾èµ–: Task 2.1, 2.2, 2.3

- [ ] **Task 3.2**: åˆ›å»º `catboost_enhanced/daily_ml_scanner.py`
  - ç”Ÿæˆä¸¤å¥—æ¨èæ¸…å• (ml_enhanced + catboost_enhanced)
  - è¾“å‡º: `ml_daily_summary.md` + `catboost_daily_summary.md`
  - ä¾èµ–: Task 2.2

- [ ] **Task 3.3**: ä¼˜åŒ– cron ä»»åŠ¡é…ç½®
  - æ›´æ–° crontab æˆ–ä»»åŠ¡è°ƒåº¦é…ç½®
  - ç¡®ä¿ weekly_retrain å’Œ daily_scanner é…åˆå·¥ä½œ
  - ä¾èµ–: Task 3.1, 3.2

### ç¬¬å››é˜¶æ®µ: éªŒè¯ä¸å¯¹æ¯” (Week 4)

- [ ] **Task 4.1**: å¯¹æ¯” ml_enhanced vs catboost_enhanced
  - å›æµ‹æ€§èƒ½å¯¹æ¯” (Sharpe, Max DD, Win Rate ç­‰)
  - æ ·æœ¬æƒé‡åˆ†æ (æƒé‡åˆ†å¸ƒ, æ ‡ç­¾åˆ†å¸ƒ)
  - ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
  - è¾“å‡º: å¯¹æ¯”æŠ¥å‘Š + å¯è§†åŒ–

- [ ] **Task 4.2**: æ€§èƒ½ä¼˜åŒ–ä¸è°ƒå‚
  - æ ¹æ®å¯¹æ¯”ç»“æœè°ƒæ•´ embargo_pct, class_weights, learning_rate ç­‰
  - æµ‹è¯•ä¸åŒçš„ ordinal loss è®¾è®¡

- [ ] **Task 4.3**: æ–‡æ¡£å’Œç›‘æ§
  - æ›´æ–° README, DESIGN_PLAN
  - è®¾ç½®ç›‘æ§å‘Šè­¦ (æ¨¡å‹æ¼‚ç§»æ£€æµ‹)

---

## âš ï¸ æ”¹è¿›å»ºè®®ä¸é£é™©ç‚¹

### é«˜ä¼˜å…ˆçº§ (å¿…é¡»å¤„ç†)

1. **ç‰¹å¾å¯¹é½** (Task 1.3, 2.1)
   - é£é™©: ml_enhanced å’Œ catboost_enhanced çš„ç‰¹å¾åˆ—ä¸åŒæ­¥
   - å¯¹ç­–:
     - ä½¿ç”¨å•ä¸€çš„ FEATURE_COLS å®šä¹‰ (via config)
     - åœ¨ prepare_*_data.py å¼€å§‹æ—¶éªŒè¯ç‰¹å¾åˆ—é¡ºåºå’Œç±»å‹
     - ä¿å­˜ feature_info.pkl (åç§° + dtype + é¡ºåº)

2. **æ•°æ®æ³„æ¼éªŒè¯** (Task 1.3, 2.2)
   - é£é™©: PurgedGroupKFold å®ç°æœ‰ bug, embargo æ²¡æœ‰çœŸæ­£ç”Ÿæ•ˆ
   - å¯¹ç­–:
     - å®ç°éªŒè¯å‡½æ•° (è§ P1 è®¾è®¡ä¸­çš„ assert gap >= embargo_days)
     - åœ¨æ¯æ¬¡è®­ç»ƒå‰æ‰“å° fold çš„æ—¥æœŸèŒƒå›´å’Œ gap
     - å¯¹æ¯”æœ‰ embargo vs æ—  embargo çš„ CV ç»“æœå·®å¼‚

3. **æ ·æœ¬æƒé‡åˆ†å¸ƒ** (Task 1.3, 2.2)
   - é£é™©: compute_sample_weights ç®—æ³•ä¸ç¨³å®š, å¯¼è‡´è®­ç»ƒå‘æ•£
   - å¯¹ç­–:
     - å®ç° verify_sample_weights() å‡½æ•° (è§ P2 è®¾è®¡)
     - åœ¨è®­ç»ƒå‰æ‰“å°æƒé‡ç»Ÿè®¡: mean, std, min, max, per-label
     - å¯¹æç«¯æƒé‡çš„æ ·æœ¬è¿›è¡Œæ£€æŸ¥ (å¦‚ |score| å¾ˆå¤§çš„æ ·æœ¬)

### ä¸­ä¼˜å…ˆçº§ (ä¼˜åŒ–ç©ºé—´)

4. **CV ç­–ç•¥çš„ä¸€è‡´æ€§** (Task 2.2 vs ml_enhanced çš„ train_models.py)
   - ml_enhanced å½“å‰: 3month window + 1month test (ä¸åŒ K å€¼)
   - catboost: PurgedGroupKFold (K=5)
   - å»ºè®®: åœ¨å¯¹æ¯”æ—¶éƒ½ç”¨åŒä¸€ä¸ª CV ç­–ç•¥

5. **Ordinal Loss** (Task 1.3)
   - å½“å‰: ä»…é€šè¿‡ sample_weight æ¨¡æ‹Ÿ
   - å¯é€‰: å®ç°å®Œæ•´çš„ ordinal loss (éœ€è¦è‡ªå®šä¹‰ gradient)
   - ä¸ç´§æ€¥: å…ˆéªŒè¯ sample_weight çš„æ•ˆæœ

6. **ç±»æƒé‡çš„è‡ªé€‚åº”è°ƒæ•´** (Task 1.2)
   - å½“å‰: ç¡¬ç¼–ç  [1.0, 1.0, 1.5, 2.0]
   - æ”¹è¿›: åŸºäºå®é™…ç±»é¢‘ç‡åŠ¨æ€è°ƒæ•´
   - ä¾‹å¦‚:
     ```python
     class_freq = df['label'].value_counts()
     class_weights = {
         0: 1.0 / (class_freq[0] + 1e-5),
         1: 1.0 / (class_freq[1] + 1e-5),
         2: 1.5 / (class_freq[2] + 1e-5),
         3: 2.0 / (class_freq[3] + 1e-5),
     }
     # æ ‡å‡†åŒ–ä½¿ mean = 1.0
     ```

### ä½ä¼˜å…ˆçº§ (é•¿æœŸ)

7. **ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–** (Task 2.1)
   - æ–°å¢ momentum indicators, market regime features
   - ä½†å‰æœŸä¸æ¨è: å…ˆéªŒè¯æ ¸å¿ƒ P0/P1/P2 çš„æ•ˆæœ

8. **Hyperparameter Tuning** (Task 4.2)
   - ä½¿ç”¨ Optuna æˆ– GridSearch è‡ªåŠ¨è°ƒå‚
   - ç›®æ ‡: maximize NDCG@10 æˆ– Sharpe ratio

---

## ğŸ“Š é¢„æœŸå›æµ‹æ•ˆæœ

åŸºäºé‡æ„è®¾è®¡, é¢„æœŸä»¥ä¸‹æ”¹è¿›:

| æŒ‡æ ‡ | ml_enhanced | catboost_enhanced | æ”¹è¿› |
|------|-----------|------------------|------|
| **æ ·æœ¬é‡** | 3000/model | 36,823 global | 12å€ |
| **è¿‡æ‹Ÿåˆå€¾å‘** | ä¸­ç­‰ | ä½ | âœ“ |
| **æ•°æ®æ³„æ¼** | å¯èƒ½å­˜åœ¨ | é¿å… (embargo) | âœ“ |
| **Aç±»æ•æ‰ç‡** | ~50% | 60%+ (ç›®æ ‡) | âœ“ |
| **è™šå‡ä¿¡å·ç‡** | ~30% | 20%+ (ç›®æ ‡) | âœ“ |
| **Sharpe Ratio** | baseline | baseline * 1.05-1.15 | âœ“ |
| **Max Drawdown** | baseline | baseline * 0.9-0.95 | âœ“ |

---

## âœ… éªŒæ”¶æ ‡å‡†

### Task å®Œæˆæ ‡å‡†

æ¯ä¸ª Task å®Œæˆæ—¶åº”æ»¡è¶³:

1. **ä»£ç è´¨é‡**
   - æœ‰æ¸…æ™°çš„ docstring å’Œæ³¨é‡Š
   - éµå¾ªé¡¹ç›®çš„ code style (è§ CLAUDE.md)
   - é€šè¿‡ linting æ£€æŸ¥

2. **åŠŸèƒ½æ­£ç¡®æ€§**
   - å•å…ƒæµ‹è¯•é€šè¿‡ (ä¸»è¦å‡½æ•°)
   - è¾“å‡ºæ–‡ä»¶çš„æ ¼å¼å’Œå†…å®¹ç¬¦åˆé¢„æœŸ
   - ä¸ ml_enhanced çš„å¯¹åº”éƒ¨åˆ†å¯¹é½

3. **æ€§èƒ½åŸºå‡†**
   - ç‰¹å¾å·¥ç¨‹æ—¶é—´ < 30s
   - æ¨¡å‹è®­ç»ƒæ—¶é—´ < 5 åˆ†é’Ÿ (å« CV)
   - æ—¥å¸¸æ‰«ææ—¶é—´ < 2 åˆ†é’Ÿ

4. **æ–‡æ¡£**
   - æ›´æ–° README æˆ–ä»»åŠ¡æ–‡æ¡£
   - è®°å½•å…³é”®è®¾è®¡å†³ç­–å’Œå‚æ•°

### å…¨é¡¹ç›®å®Œæˆæ ‡å‡†

1. ä¸¤å¥—ç³»ç»Ÿ (ml_enhanced + catboost_enhanced) çš„æ—¥æŠ¥å‘Šä¸€è‡´æ€§ > 90%
2. å›æµ‹ Sharpe ratio å¯¹æ¯”: catboost >= ml_enhanced * 0.95 (ä¸å·®äº 5%)
3. è‡ªåŠ¨åŒ–æˆåŠŸç‡: weekly_retrain å’Œ daily_scanner è¿ç»­ 4 å‘¨æ— å¤±è´¥
4. ä»£ç é‡å¤åº¦: < 20% (é€šè¿‡ src/ml çš„å…±äº«åº“é™ä½)

---

## ğŸ“š å‚è€ƒèµ„æº

### ç›¸å…³æ–‡ä»¶
- `ml_enhanced/scripts/prepare_ml_data.py` - score/label è®¡ç®—å‚è€ƒ
- `ml_enhanced/scripts/train_models.py` - feature_cols å®šä¹‰
- `catboost_enhanced/scripts/train.py` - ç°æœ‰ PurgedGroupKFold å®ç°

### CatBoost å®˜æ–¹æ–‡æ¡£
- [CatBoost MultiClass](https://catboost.ai/docs/concepts/loss-functions-multiclass)
- [CatBoost Categorical Features](https://catboost.ai/docs/concepts/categorical-features)
- [CatBoost Custom Metrics](https://catboost.ai/docs/concepts/custom-metric)

### é‡åŒ–é‡‘èå‚è€ƒ
- Purged K-Fold: LopÃ©z de Prado, M. (2018). Advances in Financial Machine Learning
- Sample Weighting: Bergstra & Bengio (2012). Random Search for Hyper-Parameter Optimization

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2025-11-22
**æ‰€æœ‰è€…**: ç”¨æˆ·
**çŠ¶æ€**: å¾…å®¡æ‰¹ä¸æ‰§è¡Œ
